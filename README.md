# Attention is All You Need Implementation
To practice and better understand the Transformer architecture, I am rebuilding the model from scratch.

This implementation is done by only reading through the paper itself (using an LLM to build this would not help me learn and would also be super ironic)

# Motivation

Nowadays the pace at which AI/ML research is progressing makes it so that there is always something new to learn.

There's always a newer model architecture or new way to optimize an LLM.

I certainly felt this way after working in research - I was focused on building a model using the latest and greatest model since that's the nature of research.

I feel like while it is certainly important to learn those, if you dive into it without fully understanding the fundamentals, there might be holes in your thinking.
