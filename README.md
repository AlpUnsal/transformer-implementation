# Attention is All You Need Implementation
To practice and better understand the Transformer architecture, I am rebuilding the model architecture from scratch.

This implementation is done by only reading through the paper itself.

I didn't end up training the model since I'm a broke undergrad student...

I looked through the Transformer implementation and only used LLMs to validate my implementation.

# Motivation

Nowadays the pace at which AI/ML research is progressing makes it so that there is always something new to learn.

There's always a newer model architecture or new way to optimize an LLM.

I certainly felt this way after working in research - there is always a newer, better model to learn and use.

I feel like while it is certainly important to learn the new advancements, not learning the fundamentals can put you at a disadvantage.

# Findings

This was a really fun project and I really got into it.

I found myself being addicted to figuring out how something worked, even when I closed my laptop.

I learned a lot about the entire flow of a transformer: embedding --> encoder --> decoder --> output.

I also had to really understand the literature to be able to code this. It was refreshing to be stuck on a problem and not have an immediate answer.